CS 162 PROJECT 1: THREADS DESIGN DOCUMENT  
 
                  
---- GROUP ----

Daniel Dobalian <ddobalian@berkeley.edu > Kwanho Ryu <ryu7249125@berkeley.edu>
Shawn D’Souza <1dsouzas@berkeley.edu > Daniela Kim <danielakim@berkeley.edu>

---- PRELIMINARIES ----


How do we get things on the ready list?  Where does the thread get unblocked?
If threads have equal priority how does scheduler pick one over the other?  Does
the L thread give up its lock when it’s executed or do we need to give up the
lock before executing H thread?  On question B6, confused about multiple threads
running on the same function and how a lock may or may not prevent this.  Help
in general race conditions. 


>> If you have any preliminary comments on your submission, notes for the TAs,
    >> or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while preparing your
>> submission, other than the Pintos documentation, course text, lecture notes,
    >> and course staff.

                 ALARM CLOCK ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or `struct'
>> member, global or static variable, `typedef', or enumeration.  Identify the
>> purpose of each in 25 words or less.

struct list_elem { int64_t end_time; /* End global tick time */ struct thread
    curr_thread; /* asleep thread */ } /* inside timer.c to define a node inside
                                          the linked list sleep_list */

static struct list sleep_list; /* global var in timer.c to record which threads
                                  are asleep */

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(), including the
>> effects of the timer interrupt handler.


  timer_sleep() begins by calculating the end time of the sleep.  This is done
  by calling timer_ticks() in thread.c and adding this with the amount of ticks
  that is passed in when the function is called.  Then we set end_time field to
  store this value. We insert the list_elem in the order of its end_sleep into
  the sleep_list using the list_insert_ordered() function.  Block this thread.  

Now we also need to make a few changes to the timer interrupt handler.  We now
need to check the linked list from earlier and compare the end tick times with
the current time from timer_ticks().  We will take all nodes that have already
passed their end times or until we run out of sleep threads, and send it to the
ready queue. 

>> A3: What steps are taken to minimize the amount of time spent in the timer
>> interrupt handler?

Our linked list is inserting in order so when the interrupt handler needs to
take whatever is done we can simply list_pop_front() and those elements will be
the expired threads. So this implementation only needs to check what is ready,
    and one additional non-ready thread or reach the end of the linked list.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call timer_sleep()
    >> simultaneously?


Calculating anything involved with ticks is not shared amongst threads is not
shared in anyway. The linked list will be shared, so protection will need to
happen when doing operations on it.  Since it is not atomic, we will block
interrupts while performing operations, then turn off interrupts afterwards. 

>> A5: How are race conditions avoided when a timer interrupt occurs during a
>> call to timer_sleep()?

Similar approach to before, turn off interrupts for the list operations and
critical operations. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to another
>> design you considered?

This design takes advantage of down time the CPU will have instead of sitting on
a thread waiting for the sleep to end.  It is a sense, the same idea of
constantly waiting, except the time it checks are more spaced out.  This
optimizes operations since we can run other threads while waiting.  We also made
the conscious choice to store the end times instead of the time remaining
because we would not need to keep updating values.  It makes sense to store an
absolute time over a relevant one.  Finally, it’s clear to reducing the number
of iterations through the linked list done by the interrupt handler is
accomplished by keeping it ordered by end time.  

             PRIORITY SCHEDULING ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or `struct'
>> member, global or static variable, `typedef', or enumeration.  Identify the
>> purpose of each in 25 words or less.

static struct list_elem ready_list_node { int priority; } /* List of processes
                                                             in THREAD_READY
                                                             state, that is,
                                                             processes that are
                                                             ready to run but
                                                             not actually
                                                             running */

static struct list dependencies;

static struct list_elem dependencies_node { static struct list_elem parent;
    static struct list first_child; int priority; tid_t thread_id; } /*
                                                                         Dependencies
                                                                         is a 2D
                                                                         linked-list
                                                                         that
                                                                         keeps
                                                                         track
                                                                         of all
                                                                         of our
                                                                         dependencies.
                                                                         At a
                                                                         high
                                                                         level,
                                                                         it
                                                                         replicates
                                                                         the
                                                                         structure
                                                                         and
                                                                         functionality
                                                                         of a
                                                                         SibTree
                                                                         as a
                                                                         linked
                                                                         list.
                                                                         When
                                                                         threads
                                                                         have no
                                                                         parent
                                                                         dependencies,
                                                                         and
                                                                         release
                                                                         their
                                                                         locks,
                                                                         they
                                                                         pop off
                                                                         this
                                                                         dependencies
                                                                         list,
                                                                         and are
                                                                         added
                                                                         to the
                                                                         ready
                                                                         list.
                                                                      */

>> B2: Explain the data structure used to track priority donation.  Use ASCII
>> art to diagram a nested donation.  (Alternately, submit a .png file.)

[See b2datastructure.png file for diagram]

Explanation of diagram: This diagram portrays the state of the elems directly
after the elem for Thread B has been added with priority 30. What would happen
after the state shown is that that elem will check all of its parents to make
sure that none of them have a priority less than 30. If one of its parents does
have a priority less than 30, which it does in this case, that value is
propagated up to the parent, so the priority of the parent elem for Thread A
will be set to 30 as well.

Adding elems: When a lock is requested by a thread, if that lock is not being
held by any other thread it will be create a new node in the top level list.
Otherwise, it will be a new child node of the thread that currently has that
lock.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for a lock,
    >> semaphore, or condition variable wakes up first?
  The threads are placed in the ready_list using thread_unblock() and each
  list_elem that represents the thread has a field of the thread’s priority.
  Using the function list_max() we’re able to always pick the thread with the
  highest priority to run next. This thread will then search through its
  dependency parents to resolve any nesting issues.

>> B4: Describe the sequence of events when a call to lock_acquire() causes a
>> priority donation.  How is nested donation handled?

When a call to lock_acquire(lock) causes a priority donation is when a
thread(thread-2) tries to hold a lock which is already held by other
thread(thread-1) whose priority is lower than thread-2’s priority. In
consequence, thread-2 with CPU is stuck just waiting for thread-1 to unlock
which will never happen because of its low priority. Thus, priority donation
takes place here and thread-1 gets high enough priority so that CPU is yielded
to thread-1 and get out of the dead end.  In the case of nested donation, we use
the data structure explained above to keep track of the complex dependences
between threads. For instance, thread-2 is depend on thread-1 if thread-2 waits
for thread-1 to unlock. If multiple threads want a lock, all those thread has
dependency with the thread that holds the lock.  Whenever a thread tries to hold
a lock which is already held by other, we find out which thread is holding the
lock by checking lock->holder, and add a link(dependency) between the
nodes(holder thread and want-to-hold thread). Then the most nested node passes
its priority to its parent and update parent’s priority if priority passed from
child is greater than itself’s priority. By doing this we ensure most
shallow(actually holding lock) node gets high enough priority and gets CPU. Then
the chain of dependencies will be resolved one by one.

>> B5: Describe the sequence of events when lock_release() is called on a lock
>> that a higher-priority thread is waiting for.
When the lock_release() is called, the thread (let’s call it L) that released
the lock removes itself from the dependency list and the child thread (let’s
        call it M) that has the highest priority now goes up one level replacing
its parent. This new thread (M) can now lock_acquire() the lock and the previous
children of (L) now are dependent on (M). This procedure can continue until a
thread does not have dependencies of the same priority as of its own.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain how your
>> implementation avoids it.  Can you use a lock to avoid this race?
  The potential race in thread_set_priority() is when some thread A wants to
  update its new_priority but before it can update its data some interrupt
  switches the current thread to some thread B. Our implementation avoids such
  race by disabling interrupts.  Using locks we can lock the data new_priority
  and the data can only be accessed when the thread who locked the data wants to
  access it.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to another
>> design you considered?
The best way we came up with to manage the lock dependencies was to use a tree
structure. Trees give clear indication of priority (based on depth) and
relationships (based on siblings vs. children). However, at the same time, we
wanted to utilize the data structures given to us. As a result, we decided to go
with SibTrees because their structure is easily replicated with nested linked
lists that have prev, next, parent, and first_child pointers.  We initially
tried approaches that involved having separate arrays for each lock. However,
      arrays failed to accurately present the dependence relationships between
      threads based on the locks that they wanted to acquire, particularly if
      such dependencies chains involved more than one lock. As well, for large
      depths, using arrays was inefficient because they would require iterating
      through many elements in many different arrays.

              ADVANCED SCHEDULER ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or `struct'
>> member, global or static variable, `typedef', or enumeration.  Identify the
>> purpose of each in 25 words or less.

int cpu_load_avg; /* global variable inside thread.c */

int nice; /* inside the thread struct */

int recent_cpu; /* inside the thread struct, used to recalculate new recent_cpu
                 */

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each has a
>> recent_cpu value of 0.  Fill in the table below showing the scheduling
>> decision and the priority and recent_cpu values for each thread after each
>> given number of timer ticks:


/* formula priority = PRI_MAX − (recent_cpu/4) − (nice ∗ 2) */
timer  recent_cpu  priority   thread
ticks   A   B   C   A   B   C   to run
-----    --   --   --    --  --    --   ------
 0       0    0   0   63 61  59     A
 4       4    0   0   62 61  59     A
 8       8    0   0   61 61  59     B
12      8    4   0   61 60  59     A
16     12   4   0   60 60  59     B
20     12   8   0   60 59  59     A
24     16   8   0   59 59  59     C
28     16   8   4   59 59  58     B
32     16  12  4   59 58  58     A
36     20  12  4   58 58  58     C

>> C3: Did any ambiguities in the scheduler specification make values in the
>> table uncertain?  If so, what rule did you use to resolve them?  Does this
>> match the behavior of your scheduler?
Yes, the spec does not indicate how to handle a tie.  As per the recommendation
of lecture, we want to make sure each thread is progressing so in the case of a
tie it is broken by selecting whichever thread has recent_cpu time.  

>> C4: How is the way you divided the cost of scheduling between code inside and
>> outside interrupt context likely to affect performance?

We compute the recent_cpu and niceness in an interrupt context while otherwise
periodically computing recent_cpu (re-computing) and load_avg outside the
interrupt context. Specifically, every time the interrupt context is called, we
generate recent_cpu and niceness, use these values to then compute an updated
recent_cpu, and then store them in the threads struct.  Alternatively, priority
is re-calculated every 4 ticks to be used, but it is not being stored.  This is
done using the values we have been updating, and then can be used to determine a
priority.  Once the priority is calculated, it can be used to determine the
thread to run (with the occasional use of recent_cpu to break ties).  This
process takes advantage of the fact we are not required to keep an update
priority, but only change it every 4 ticks.  Moreover, we calculate load_avg
outside of the interrupt context to reduce computation.  Unfortunately, the doc
confines us to compute recent_cpu inside the interrupt context. 

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and disadvantages
>> in your design choices.  If you were to have extra time to work on this part
>> of the project, how might you choose to refine or improve your design?
The disadvantage of our design is that we simply assume that performing
computations is faster than storing and fetching from memory but in the case
that we have a fast read and write from memory we would not be optimizing that.
The advantages of our design choices is that we are only storing the necessary
values and are minimizing I/O.  If we had more time, we would test out how the
different data structure implementations affect performance under different
hardware and load configurations. For example, it would be suboptimal to store
less and compute more on a system with fast memory and a slow processor. We’d
also want to test our code against scenarios of varying levels of load
balancing.

